---
layout: default
title: "Overlap Measure"
permalink: /docs/sampling/2025-05-27overlap
nav_order: 3
parent: Sampling
children: False
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['\\(','\\)'], ['$', '$']]
      }
    });
  </script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>

# Overlap

Throughout this analysis, there's been mention to the notion of overlap between two distributions. The goal of this section is to define overlap and how we can leverage its interpretation in application over the commonly used p-value.


## Overlap in Rejection Sampling
When describing the rejection sampling algorithm, we showed that we could define the probability a sample was accepted as $\frac{1}{M}$. Here the constant M is defined as:
$$\sup_{x \in \Omega}\frac{q(x)}{p(x)}$$
Note that if $q(x) = p(x)$ then the sample would always be accepted. 

Similarly, if $q(x) = c p(x)$ where c is a constant, then the probability of acceptance would become $\frac{1}{c}$. Of course, this definition leads to conclusion that if two distributions are more "overlapping", then the probability of acceptance draws closer to 1 (Figure add). The challenge with rejection sampling is $M < \infty$. Often times, M being less than infinity is not the case removing both the interpretation of overlap and rendering rejection sampling useless. We'd like to find a way to remove this unnecessary constraint and find a definition of overlap that works for any $p(x)$, $q(x)$.

## Overlap in Metropolis-Hastings
The second sampling algorithm we analyzed was the Metropolis-Hastings algorithm. One of the key properties of MH we derived was that it guarantees convergence to the stationary distribution under the condition of detailed balance. This provides an algorithm that works for any $p(x),  q(x)$ regardless of how slow that convergence may be. This provides a significant advantage over Rejection Sampling since we can define overlap for any $p(x)$ and $q(x)$. However, the challenge now is defining overlap in the MH algorithm.

<br>

**Claim:**
 The integral $\int \int \alpha(x',x)p(x)q(x')\text{d}x\text{d}x'$ gives us a measure of overlap of the MH algorithm.


<br>
We'll define Z to be a Bernoulli random variable indicating whether or not our sample was rejected and we'll start with the following set up:

Suppose a target distribution $q(x)$ and a proposal distribution $p(x)$.
Let $X \sim q(x)$ and $Y \sim p(y)$. We'd like to know the probability X can replace Y after the test: $u < \alpha(X, Y)$.

Define $z$ to be a r.v st:
$$X' = \begin{cases}
X \text{ with probability } \alpha(X, Y) \\
Y \text{ with probability } 1-\alpha(X, Y)
\end{cases}
$$
<br>

We'd like show $X' \sim q(x)$. 

Clearly if we reject our sample $X$, $X' \sim q$ since $X' = Y$. If we accept the sample, then we were saying $X$ can replace $Y$. We can make the following observation of the probability of replacement:

$$\Pr(Z=1 \cap X' \in A | X) = \int \int \int_A 1(U < \alpha(X, Y)) q(x)p(y)\text{d}u\text{d}x\text{d}y$$

$$= \int \int \alpha(x,y)q(x)p(y)\text{d}x\text{d}y$$

Where $A$ is the sample space of $q(x)$. This is our overlap measure, we'll denote it with $r$.

$$r = \int \int \alpha(x,y)q(x)p(y)\text{d}x\text{d}y$$

Since r is a well defined probability, $r \in [0,1]$ and has the following interpretation:


<ol>
   <li>If r is close to zero, that indicates the target and proposal are distinct distributions since there is a low replacement probability.
    <li>If r is close to one, then the proposal and distinct are similar distributions.
    <li>Computing r directly may not be always possible, the probability must be estimated using Monte Carlo methods.
</ol>

<br>
In conclusion, since the Metropolis Hastings converges to the target distribution for any proposal distribution, the overlap measure for the Metropolis Hastings works for any two $p(x)$ and $q(x)$. This allows us to determine the similarity between samples coming from two different distributions by determining how likely one sample is to replace the other.


## Similarity measures: p-value vs Overlap
The p-value is the probability a test statistic is at least as extreme as the actual observed value, given that a null hypothesis is true. One common application of the p-value is to test the differences between two populations (X and Y) by using the means from the observed samples and constructing the following null hypothesis:

$$
H_0 \colon \ \mu_X - \mu_Y = 0 \quad
H_1 \colon \ \mu_X - \mu_Y \neq 0
$$

We arrive at what is referred to as the two sample t-test. By rejecting the null hypothesis, we accept strong evidence to suggest the means are not the same, indicating a difference between the two populations. The threshold for p-value (commonly referred to as $\alpha$) is selected based on a widely accepted consensus, typically .05. However, given the goal is to measure differences between two populations, the threshold we set for the p-value is uninformative about the difference between the two populations.

Instead of the p-value, we'd like to focus on $r$ and its interpretation as a measure of replacement between two populations to describe similarity between distinct distributions.


### Example: p-value vs. Overlap
For this example, we'll consider two populations:
$$X_1, X_2, ..., X_N \text{ i.i.d from N(0,1)} $$
and
$$Y_1, Y_2, ..., Y_N \text{ i.i.d from N(}\mu \text{,1)}$$
where $\mu \in [0,2]$. We'd like to compare the similarity of the two distributions using both a two sample mean test and $r$ and highlight the difference between the two measures as $\mu$ grows larger and the distributions grow farther apart. Since $r$ cannot be directly computed, it must be estimated using Monte Carlo methods. We'll estimate r using the following code:
```
import pandas as pd
import scipy
import numpy as np
# Generate data
np.random.seed(42)
# For reproducibility
means = np.arange(0, 200) / 100
x = scipy.stats.norm.rvs(size=50)
y = scipy.stats.norm.rvs(size=50)

# Fit KDE for x (fixed)
fx_hat = scipy.stats.gaussian_kde(x)
fx_s = fx_hat.resample(1000).flatten()  # Pre-sample

overlaps = []
p_values = []

for mu in means:
    y_mu = y + mu
    
    # Fit KDE for shifted y
    fy_hat = scipy.stats.gaussian_kde(y_mu)
    #Drawing 1000 samples from the fy hat
    fy_s = fy_hat.resample(1000).flatten()
    
    # Two-sample t-test (two-sided by default)
    tstat, p_val = scipy.stats.ttest_ind(x, y_mu, equal_var=False)  # Welch's t-test
    p_values.append(p_val)
    
    # Monte Carlo overlap estimation
    eps = 1e-10  # Small constant to avoid division by zero
    numerator = fx_hat(fy_s) * fy_hat(fx_s)
    denom = fx_hat(fx_s) * fy_hat(fy_s)
    alpha = np.minimum(1, (numerator / denom))
    overlaps.append(np.mean(alpha))

# Store results
df = pd.DataFrame({'Overlap': overlaps, 'p-value': p_values}, index=means)
```
We obtain the following result:
<p align="center">
   <img src="/assets/overlap (normal).png" alt="Overlap normal ex" width="300"/>
</p>   


From this example we see as the p-value dips below .05 and we begin to reject the null hypothesis, the overlap remains high at roughly .75 probability a sample from $\text{N(0,1)}$ could replace a sample from $\text{N(}\mu,1)$. From this example, we see the additional information we gain from using the overlap measure could be far more valuable to the researcher since three-fourth's of samples from one group could replace those of the other group.