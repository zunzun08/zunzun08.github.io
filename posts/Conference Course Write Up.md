# The Metropolis Hastings Algorithm

**Introduction:**
The Metropolis-Hastings (MH) algorithm is a Monte Carlo Markov Chain (MCMC) method that allows us to generate samples from a target probability distribution of interest. It works by selecting two probability density functions (pdf): one pdf represents the target distribution and the other a proposal distribution. Through an iterative process, samples are drawn from the proposal pdf, and are then accepted or rejected based on a criterion that ensures the final collection of samples approximate the target distribution. After completing the process, the MH algorithm simulates sampling from the target distribution without the use of sampling from the target distribution.

**Example:**
Suppose $p(x) \sim N(0,1)$ and $q(x) \sim N(1,2)$ where $p(x)$ is a distribution that is easy to sample from and $q(x)$ is difficult to sample from. By applying the MH Algorithm 10,000 times, and only sampling from $p(x)$, we can achieve the following result:
![/Users/cristianzuniga/Downloads/metropolis_hastings-ezgif.com-video-to-gif-converter.gif](file:///Users/cristianzuniga/Downloads/metropolis_hastings-ezgif.com-video-to-gif-converter.gif)
Effectively, we are taking samples from $p(x)$ and concluding that they are in fact coming from $q(x)$.

### Mathematical Background:
Building up to the MH algorithm requires we define a few concepts from Stochastic Processes before we begin analyzing and proving why Metropolis works.


**Monte Carlo**:
Monte Carlo methods are a class of computational techniques that use random sampling to approximate mathematical quantities, such as integrals. These methods are useful when analytical solutions are difficult or impossible to obtain.

Given a probability density function $f(x)$ and a independently and identically distributed (i.i.d) sample $x_1, x_2, ..., x_n$ drawn from $f(x)$, we can approximate the integral of a continuous function $a(x)$ with respect to $f(x)$:
$$I = \int a(x)f(x)dx$$
With the following estimation:
$$\hat{I} = \frac{1}{n} \sum_{i=1}^{n} a(x_i)$$


Where $\hat{I}$ is an unbiased estimator of $I$. 


$\{$



**Proof**:
It should be noted that by the Law of the Unconscious Statistician:
$$I = \int a(x)f(x)dx = E[a(x)]$$
By properties of expected value we apply the following:
$$E[\hat{I}] = E[\frac{1}{n} \sum_{i=1}^n a(x_i)] = \frac{1}{n} \sum_{i=1}^n E[a(x_i)]$$
Since $x_1, x_2, ..., x_n$ are i.i.d
$$E[\hat{I}] = \frac{1}{n} (nI) = I = E[a(x)]$$

Now we want to show that as the number of simulations, n, increase, we want to show that the variance of $\hat{I}$, converges to zero. We start by computing $E[I^2]$
$$E[(a(x))^2] =\int (a(x))^2f(x)dx$$
$$E[\frac{1}{n^2} (\sum_{i=1}^{n} a(x_i))^2] $$
Using the variance formula: $\text{Var}[I] = E[I^2] - (E[I])^2$
$$\text{Var}[I] = \frac{\hat{I}^2}{n} - \hat{I^2} = \hat{I}^2(\frac{1}{n} - 1)$$
Now lets take the limit as n approaches infinity:
$$\lim_{n \rightarrow \infty}{\text{Var[I]}} = \lim_{n \rightarrow \infty}{\hat{I}^2 (\frac{1}{n} - 1)} = \hat{I}^2 (\lim_{n \rightarrow \infty}{(\frac{1}n - 1)}) = \hat{I}^2 $$




**Markov Chain Stationarity Property:**
A Markov chain is a stochastic process where the future state depends only on the current state and not on the sequence of events that preceded it. A key concept in Markov chains is the **stationary distribution**, which is a probability distribution that remains unchanged as the chain evolves over time.

The MC Stationarity property requires the following:
1. A p.d.f: $f(x)$
2. An i.i.d sample $x_1, x_2, ..., x_n$ drawn from $\Pr(\cdot | x_{i-1})$, the transition kernel from the previous state, $x_{i-1}$.
3. The stationary distribution must satisfy the following process: 
$$f(y) = \int \Pr(y | x) f(x) \text{dx}$$
If all three conditions are met then the following is true:


If the initial state $x_1$​ is drawn from the stationary distribution $f(x)$, and each subsequent state $x_{i+1}$ is drawn from $\text{Pr}⁡(⋅∣x_i)$, then all states $x_i$ will also follow the stationary distribution $f(x)$. 

**Markov Chain Monte Carlo:**
Markov Chain Monte Carlo (MCMC) methods combine Monte Carlo sampling with Markov chains to generate samples from a target probability distribution $f(x)$.

Given a sequence of samples $x_1,x_2,…,x_n$​ generated by a Markov chain with stationary distribution $f(x)$, we can approximate expectations of a function $a(x)$ as follows:
Then:$$\Pr(\cdot | x_1) \approx \frac{1}{n} \sum_{i=1}^n a(x_i)$$This approximation holds because, under certain conditions, the Markov chain will eventually produce samples that are distributed according to $f(x)$, even if the samples are not independent.

By the definition above and the description above, the MH algorithm is an MCMC method.


## The Algorithm
**Metropolis-Hastings Algorithm:**
1. Pick $p(x)$ and $q(x)$ where $p(x)$ is a distribution that can be easily sampled from.
2. Set an initial value of $x$ (could be anything, keep it reasonable)
3. Take $x'$ from $p(x)$
4. Evaluate $q(x')$
5. Compute $\alpha(x', x) = \min(1, \frac{p(x')q(x)}{q(x')p(x)})$
6. Draw a random sample $u$ from $\text{Uniform}[0,1]$
7. Accept or reject: 
   If $u < \alpha(x',x)$:
	$x_2 = x'$
	else:
		$x_2 = x_1$

## Proof of Metropolis Hastings Algorithm
We start with the following idea:
Applying the MH Algorithm does the following:
$$X[1]_{X_1} \xrightarrow{\text{alg.}} X[2]_{X_2}$$
This implies that there must be a density $\text{Pr}(X_2 | X_1)$ that facilitates this movement of random variables.


We also wish to prove $q(y) = \int \Pr(y | x) q(x) dx$, the stationarity condition for our target distribution.
However, we first need to find what $Pr(x_{n+1} | x_n)$ is first before we can show the stationary condition holds

#### Finding $\Pr(X_{n+1} | X_n)$:
$X_{n+1}$ can take on two values. Either $X_{n+1}$ takes on the values of $x'$ or we choose to accept $X_n \sim q$ given we reject $x'$. 
We can represent this as:
$$X_{n+1} = \begin{cases}
x' \text{ determined by w(x)} 
\\X_n \text{ 1- w(x)}
\end{cases}$$
Where $w(x)$ is going to be our "weight" or acceptance of probability for our density function.
Since we are looking to determine $\Pr(X_{n+1} | X_n)$, using the Law of Total Probability, we get:
$$\Pr(X_{n+1} | X_n) = \text{Pr}(X_{n+1} = x' | X_{n} = x_{n} ) + (1- \text{Pr}(X_{n+1} = x' | X_{n} = x_{n} )) \cdot 1(X_{n+1} = X_n)$$
Where $1(X_{n+1} = X_n)$ is an indicator variable that tells us our sample was rejected. Let's take this apart and quantify $\Pr(X_{n+1} = x' | X_{n} = x_n)$, the probability we move from one state to the next.

$$
\Pr(X_{n+1} = x' | X_{n} = x_n) = \Pr(X' | X_n = x_n) \cdot \Pr(U \leq \alpha(x', x_n) | X' = x', X_n =x_n) =q(x' | x_n) \alpha(x', x_n) dx'
$$
Since our outcomes are binary, $X_{n+1}$ either takes on $x'$ or $X_n$, we can directly construct our density function:
$$\text{Pr}(X_{n+1} | X_{n}) = w(x)f(x_{n+1} | x_n) + (1-w(x))1(X_{n+1} = X_n) $$
where $w(x)$ is:
$$w(x_n) = \int q(x_{n+1} | x_n) \alpha(x_{n+1}, x_n)dx_{n+1}$$
and $f(x_{n+1} | x_n)$ is:
$$f(x_{n+1} | x_n) = \frac{q(x_{n+1} | x_n) \alpha(x_{n+1}, x_n)}{\int q(x_{n+1}|x_n)\alpha(x_{n+1} , x_n) dx_{n+1}
}$$

#### Detailed Balance (Reversibility):

$$q(x')\Pr(x', x) = q(x)\Pr(x,x')$$

#### Proving Stationarity:
We prove stationarity of the transition kernel by proving that the algorithm is a sufficient condition for stationarity:
We start with
$$\alpha(x', x) = \min(1,\frac{p(x')q(x|x')}{p(x)q(x'|x)}) = \min \{ \space p(x)q(x'|x), p(x')q(x|x') \space \}
\newline
$$


$$q(x'|x)\alpha(x',x)= q(x'|x)\min \{ \space p(x)q(x'|x), p(x')q(x|x') \space \} = \Pr(X_{n+1} = x' | X_{n} = x_n)$$


**USE DETAILED BALANCE HERE**




## Downsides to Metropolis Hastings:

**Autocorrelation**:
The first few thousand samples are likely to be autocorrelated even if the chain converges to $p$. This results in the first few thousand samples needing to be removed in what's referred to as a "burn in" period.

*PROVIDE EXAMPLE*


**Computationally Expensive**:
Simulations may need to be run on the order of thousands to achieve sufficient statistical power for specific tests. This could result in millions of simulations being required, leading to computational demands that exceed the capacity of a single computer. To address this, parallelization or advanced techniques such as Stochastic Gradient MCMC may be necessary.


## Applications and Extensions of Metropolis-Hastings:

### Bayesian Inference:
Stochastic Gradient MCMC - Uses small batches of data at a time to approximate the target distribution by evaluating the likelihood a random subset is representative of the data.