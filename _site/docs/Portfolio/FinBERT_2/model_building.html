<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(3) > ul > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(3) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(3) > ul > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(3) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(3) > ul.nav-list > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>FinBERT 2 - Model Building | Cristian Compean</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="FinBERT 2 - Model Building" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Portfolio website for Cristian Compean" /> <meta property="og:description" content="Portfolio website for Cristian Compean" /> <link rel="canonical" href="http://localhost:4000/docs/Portfolio/FinBERT_2/model_building" /> <meta property="og:url" content="http://localhost:4000/docs/Portfolio/FinBERT_2/model_building" /> <meta property="og:site_name" content="Cristian Compean" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="FinBERT 2 - Model Building" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Portfolio website for Cristian Compean","headline":"FinBERT 2 - Model Building","url":"http://localhost:4000/docs/Portfolio/FinBERT_2/model_building"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Cristian Compean </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Sampling category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/Blog/Sampling/2025/05/28/sampling" class="nav-list-link">Sampling</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/Blog/Sampling/2025/05/19/rejection-sampling" class="nav-list-link">Rejection Sampling</a></li><li class="nav-list-item"><a href="/docs/Blog/Sampling/2025/05/18/Metropolis-Hastings" class="nav-list-link">Metropolis-Hastings Algorithm</a></li><li class="nav-list-item"><a href="/docs/Blog/Sampling/2025/05/27/overlap" class="nav-list-link">Overlap Measure</a></li></ul></li><li class="nav-list-item"><a href="/docs/Portfolio/Overlap_RANDHIE" class="nav-list-link">Application - Overlap Measure and RAND HIE (WIP)</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in FinBERT 2 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/Portfolio/FinBERT_2" class="nav-list-link">FinBERT 2</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/Portfolio/FinBERT_2/model_building" class="nav-list-link">FinBERT 2 - Model Building</a></li></ul></li><li class="nav-list-item"><a href="/Portfolio/2025/06/10/portfolio-website-blog" class="nav-list-link">Portfolio Website Updates</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Cristian Compean" aria-label="Search Cristian Compean" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/docs/Portfolio/FinBERT_2">FinBERT 2</a></li> <li class="breadcrumb-nav-list-item"><span>FinBERT 2 - Model Building</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h2 id="initial-problem"> <a href="#initial-problem" class="anchor-heading" aria-labelledby="initial-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Initial Problem: </h2> <p>Sentiment analysis plays a crucial role in financial trading by classifying breaking news as bullish or bearish, enabling traders to adjust their buy/sell strategies accordingly. As part of a larger project to capture financial news sentiment, Iâ€™ve been experimenting with BERT (Bidirectional Encoder Representations from Transformers) as a model that can handle the complexity of financial language while avoiding the computational costs associated with billion-parameter large language models.</p> <h4 id="why-bert"> <a href="#why-bert" class="anchor-heading" aria-labelledby="why-bert"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why BERT? </h4> <p>As mentioned earlier, BERT can be relatively easy to train using Python libraries such as Huggingface and is an introductory model to language modeling that can also produce good results.</p> <h4 id="berts-not-enough"> <a href="#berts-not-enough" class="anchor-heading" aria-labelledby="berts-not-enough"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> BERTâ€™s Not Enough: </h4> <p>One problem with BERT is that since the data the model is trained only includes the Wikipedia and Book corpus corpora, BERT may have difficulty capturing the bullish or bearish sentiment of financial news since it may be unfamiliar with the terminology or semantics of financial news. One recommendation to remedy this downside is to <strong>fine-tune a BERT model</strong> on a related dataset, fine-tuning allows the model to use and adjusts its parameter weights to make a prediction on a task from a domain specific dataset. While vanilla BERT provides good results on most sentiment analysis tasks, I believe we can squeeze a bit more performance out of BERT.</p> <h4 id="further-pre-training"> <a href="#further-pre-training" class="anchor-heading" aria-labelledby="further-pre-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Further Pre-Training: </h4> <p>Another step I can take to optimizing BERTâ€™s performance is to <strong>further pre-train BERT</strong>-which is the technique of training BERT on a large, domain specific dataset using Masked Language Modeling (MLM), which is how BERT is originally trained. Essentially, the idea is to give BERT a chance to view words that may be rare in the real world but more common in your domain, so that it can adjust its parameter weights to more closely resemble the domainâ€™s distribution before BERT is fine tuned for a specific NLP task.</p> <p><strong>In this experiment, I will compare the fine-tuning results from the of two DistilBERT (a lighter BERT variant) models on the<a href="https://huggingface.co/datasets/takala/financial_phrasebank"> Financial Phrasebank dataset</a>: one vanilla DistilBERT and one DistilBERT that has been further pretrained on financial news. The comparison will help to determine whether domain-specific pretraining improves sentiment classification performance.</strong></p> <h2 id="pre-train-processes"> <a href="#pre-train-processes" class="anchor-heading" aria-labelledby="pre-train-processes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Pre-train Processes: </h2> <h3 id="pre-train-dataset"> <a href="#pre-train-dataset" class="anchor-heading" aria-labelledby="pre-train-dataset"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Pre-train Dataset </h3> <p>The data weâ€™ll be using for pre-training is a combination of two Huggingface datasets: the Reuters financial news dataset and the Financial News dataset. We can visualize the data here:</p> <div class="tableauPlaceholder" id="viz1752036764024" style="position: relative"> <noscript> <a href="#"> <img alt="Dashboard 1" src="https://public.tableau.com/static/images/Bo/Book1_17515796415940/Dashboard1/1_rss.png" style="border: none" /> </a> </noscript> <object class="tableauViz" style="display:none;"> <param name="host_url" value="https%3A%2F%2Fpublic.tableau.com%2F" /> <param name="embed_code_version" value="3" /> <param name="site_root" value="" /> <param name="name" value="Book1_17515796415940&#47;Dashboard1" /> <param name="tabs" value="no" /> <param name="toolbar" value="yes" /> <param name="static_image" value="https://public.tableau.com/static/images/Bo/Book1_17515796415940/Dashboard1/1.png" /> <param name="animate_transition" value="yes" /> <param name="display_static_image" value="yes" /> <param name="display_spinner" value="yes" /> <param name="display_overlay" value="yes" /> <param name="display_count" value="yes" /> <param name="language" value="en-US" /> <param name="filter" value="publish=yes" /> </object> </div> <script type="text/javascript"> var divElement = document.getElementById('viz1752036764024'); var vizElement = divElement.getElementsByTagName('object')[0]; if (divElement.offsetWidth > 800) { vizElement.style.minWidth = '1366px'; vizElement.style.maxWidth = '1466px'; vizElement.style.width = '100%'; vizElement.style.minHeight = '795px'; vizElement.style.maxHeight = '895px'; vizElement.style.height = (divElement.offsetWidth * 0.75) + 'px'; } else if (divElement.offsetWidth > 500) { vizElement.style.width = '100%'; vizElement.style.height = (divElement.offsetWidth * 0.75) + 'px'; } else { vizElement.style.width = '100%'; vizElement.style.height = '1527px'; } var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement); </script> <p>From this dataset, we see that 90% of the articles come from Reuters from 2006-2013. Unfortunately, this dataset isnâ€™t very new and in a real world application, a dataset formed with more recent periods (2021 forward) would be favorable due to the immense impact of COVID-19 and the higher interest rates weâ€™ve seen in the years following the pandemic. However, for this application the timeframe works just fine.</p> <h3 id="tokenization-of-articles"> <a href="#tokenization-of-articles" class="anchor-heading" aria-labelledby="tokenization-of-articles"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Tokenization of Articles </h3> <p>In order to train BERT, we first have to tokenize our dataset for BERT to be able to ingest our text data. Instead of training BERT on the full articles, Iâ€™m going to <strong>limit the article length to the first 512 words</strong> of the article. I do this for the following reason:</p> <ul> <li><strong>Major compute cost savings.</strong> Training on the full article lengths would result in days long tokenization even with the best compute engines. Since we care more about simply knowing if further pre-training has an effect, itâ€™s okay to truncate all articles to a standard length.</li> <li><strong>Choosing quantity over quality.</strong> It can be argued, and I think rightfully so, that training DistilBERT (more to come on DistilBERT) on fewer high quality articles would result in better fine-tuning results. Implementing this approach would require additional preprocessing including advanced text parsing, content classification, and possibly manual review to identify which articles to retain. Given the uncertain benefits and the time commitment required to curate such a dataset, I choose to prioritize the larger dataset for this analysis.</li> </ul> <p>Using the Hugging face library, tokenizing the dataset is easy and using Google Colab CPUs, tokenizing is fast and completed in about 4 minutes. Iâ€™ve tokenized the dataset and uploaded 3 tokenized versions:</p> <ul> <li><a href="https://huggingface.co/datasets/Czunzun/Financial_news_first_512">First 512 words</a></li> <li><a href="https://huggingface.co/datasets/Czunzun/Financial_news_middle_512">Middle 512 words</a></li> <li><a href="https://huggingface.co/datasets/Czunzun/Financial_news_first_4096">First 4096 words (specifically for a LongformBERT) </a></li> </ul> <h4 id="cost"> <a href="#cost" class="anchor-heading" aria-labelledby="cost"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Cost: </h4> <p>Hereâ€™s the associated time it took to tokenize each dataset using a L4 GPU from Google Colab:</p> <ul> <li>Financial News First 512 words: 5:52 mins</li> <li>Financial News Middle 512 words: 6:04 mins</li> <li>Financial News First 4096 words: 0:03 mins Total Compute Time: 11:59 mins Total Cost (Price * (Training Time / Total Compute Time Available)): $.042 <em>As of 07/08/2025 Google Colab Pro is $10 monthly for 100 compute credits and a L4 GPU uses about 2.09 compute units per hour</em> <h3 id="pre-training-bert"> <a href="#pre-training-bert" class="anchor-heading" aria-labelledby="pre-training-bert"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Pre-training BERT: </h3> <p>Now that our dataset is tokenized, we can train our model using MLM. MLM works by applying a mask on randomly selected tokens and having the model predict the value of that token. This is the core principle behind training BERT and was the training method used to train BERT in the paper that introduced it, Attention Is All You Need. I then apply the mask to 15% of the tokens, like in the Financial News First 512 dataset and perform a 95% train-test split for training.</p> </li> </ul> <p>You may have noticed in a previous section I said I trained a DistilBERT model instead of a BERT model. Now that weâ€™re in the training section Iâ€™ll explain why I made this choice:</p> <ul> <li><strong>Pre-training a BERT model is computationally expensive</strong> given our dataset size of over 200 million words. DistilBERT solves this problem by reducing BERTs size by 40% and retaining 97% of BERTâ€™s language understanding capabilities.</li> </ul> <h4 id="cost-1"> <a href="#cost-1" class="anchor-heading" aria-labelledby="cost-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Cost: </h4> <p>By optimizing the training arguments for speed using Huggingfaceâ€™s Training Arguments method and Google Colabâ€™s TPUs, the pre-training took about 18 minutes at a cost of $.12 (Price * (Training Time / Total Compute Time Available))</p> <p><em>As of 07/08/2025 Google Colab Pro is $10 monthly for 100 compute credits and a ve5-1 TPU uses about 4.05 compute units per hour</em></p> <p>After pre-training, I uploaded the model to Huggingface for storage. <a href="https://huggingface.co/Czunzun/finbert2_v2">You can access it here! </a></p> <h2 id="results"> <a href="#results" class="anchor-heading" aria-labelledby="results"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Results </h2> <p>Now that our model is pre-trained, its time to fine-tune the models. Given the size of the Financial Phrasebank dataset, optimizing training arguments for speed wonâ€™t be necessary. Therefore, we can keep the number of epochs at 3 and use the Adam optimizer.</p> <h4 id="loss-metrics"> <a href="#loss-metrics" class="anchor-heading" aria-labelledby="loss-metrics"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Loss Metrics: </h4> <p>The loss metrics weâ€™ll use to evaluate model performance are:</p> <ul> <li>Accuracy (0-1 range): Did the model correctly predict the label?</li> <li>Cross-Entropy Loss (0-1 range): How confident was the model in itâ€™s predictions when it was incorrect?</li> <li>F1 Score (0-1 range): What is the models predictive power?</li> </ul> <p>Here are the results:</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: left"><strong>Metrics</strong></th> <th style="text-align: center"><strong>DistilBERT</strong></th> <th style="text-align: center"><strong>FinBERTFast</strong></th> <th>Difference</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Accuracy</strong></td> <td style="text-align: center">84.02%</td> <td style="text-align: center">85.98%</td> <td><strong>+</strong>1.96%</td> </tr> <tr> <td style="text-align: left"><strong>F1 Score</strong></td> <td style="text-align: center">80.83%</td> <td style="text-align: center">84.24%</td> <td><strong>+</strong>3.41%</td> </tr> <tr> <td style="text-align: left"><strong>Cross-Entropy Loss</strong></td> <td style="text-align: center">82.37%</td> <td style="text-align: center">69.66%</td> <td><strong>-</strong>12.71%</td> </tr> </tbody> </table></div> <p>From the results, we see improvements across the board for our specialized model, FinBERTFast. The metric that gives me the most confidence in FinBERTFast is the increase in F1 Score since the increase is not so high that would suggest the model is overfitting, but suggests the model is performing better across all classes.</p> <p>One problem with FinBERTFast is that it is still overconfident in its predictions even when itâ€™s wrong. This is especially dangerous because its accuracy is not high enough to allow inaccurate predictions to be written off as one off mistakes, but the accuracy is also not low enough to discard the model.</p> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion: </h2> <p>The results prove that domain-specific pre-training delivers measurable performance improvements at minimal computational cost.</p> <p>Still, I think the following questions are worth asking:</p> <ul> <li>Did model performance improve because the model ingested more data or because the data was domain related?</li> <li>How does further pre-training affect models with more parameters that have been pre-trained on more data?</li> <li>Will continuing to add more data improve the results of FinBERTFast?</li> <li>Why is FinBERTFast so overconfident in its predictions?</li> </ul> <p>For a definitive answer to whether or not further pre-training improves model performance, we need to increase the number of parameters in our model and follow the same process above.</p> <p>That leaves an exciting next step for this project!</p> <p>For now, weâ€™ve found a way to improve the results of DistilBERT for our needs!</p> <p>Long live DistilBERTFast!</p> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
