<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li:not(:nth-child(2)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(2) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(2) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list > li.nav-list-item:nth-child(2) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Metropolis-Hastings Algorithm | Cristian Compean</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="Metropolis-Hastings Algorithm" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Portfolio website for Cristian Compean" /> <meta property="og:description" content="Portfolio website for Cristian Compean" /> <link rel="canonical" href="http://localhost:4000/docs/Blog/Sampling/2025/05/18/Metropolis-Hastings" /> <meta property="og:url" content="http://localhost:4000/docs/Blog/Sampling/2025/05/18/Metropolis-Hastings" /> <meta property="og:site_name" content="Cristian Compean" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Metropolis-Hastings Algorithm" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Portfolio website for Cristian Compean","headline":"Metropolis-Hastings Algorithm","url":"http://localhost:4000/docs/Blog/Sampling/2025/05/18/Metropolis-Hastings"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Cristian Compean </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Sampling category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/Blog/Sampling/2025/05/28/sampling" class="nav-list-link">Sampling</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/Blog/Sampling/2025/05/19/rejection-sampling" class="nav-list-link">Rejection Sampling</a></li><li class="nav-list-item"><a href="/docs/Blog/Sampling/2025/05/18/Metropolis-Hastings" class="nav-list-link">Metropolis-Hastings Algorithm</a></li><li class="nav-list-item"><a href="/docs/Blog/Sampling/2025/05/27/overlap" class="nav-list-link">Overlap Measure</a></li></ul></li><li class="nav-list-item"><a href="/docs/Portfolio/Overlap_RANDHIE" class="nav-list-link">Application - Overlap Measure and RAND HIE</a></li><li class="nav-list-item"><a href="/docs/Portfolio/FinBERT_2" class="nav-list-link">FinBERT 2</a></li><li class="nav-list-item"><a href="/docs/Portfolio/FinBERT%202/2025-06-19model_building.html" class="nav-list-link">Introduction to Sentiment Analysis</a></li><li class="nav-list-item"><a href="/Portfolio/2025/06/10/portfolio-website-blog" class="nav-list-link">Portfolio Website Updates</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Cristian Compean" aria-label="Search Cristian Compean" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/docs/Blog/Sampling/2025/05/28/sampling">Sampling</a></li> <li class="breadcrumb-nav-list-item"><span>Metropolis-Hastings Algorithm</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['\\(','\\)'], ['$', '$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <h1 id="the-metropolis-hastings-algorithm"> <a href="#the-metropolis-hastings-algorithm" class="anchor-heading" aria-labelledby="the-metropolis-hastings-algorithm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Metropolis-Hastings Algorithm </h1> <h2 id="introduction"> <a href="#introduction" class="anchor-heading" aria-labelledby="introduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction </h2> <p>The Metropolis-Hastings (MH) algorithm is a Monte Carlo Markov Chain (MCMC) method that allows us to generate samples from a target distribution. A target distribution is a function whose form is known but whose constant of proportionality is unknown and/or cannot be calculated through traditional methods or numerical methods. Because the constant of proportionality cannot be calculated, traditional methods of sampling such as the inverse CDF method cannot be applied. We’ll assume that the rejection sampling method does not work here for our proposal and target distributions. Instead, we turn to the Metropolis Hastings algorithm which works by also works by an iterative process. The Metropolis Hastings algorithm draws samples from a proposal probability density function and based on an overlap measure, the sample is accepted or rejected. After some number of iterations, the samples accepted by the MH algorithm follow the target distribution without ever having to calculate the constant of integration.</p> <h3 id="example"> <a href="#example" class="anchor-heading" aria-labelledby="example"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Example </h3> <p>Suppose $q(x) \sim N(0,1)$ and $p(x) \sim N(1,2)$ where $p(x)$ is a distribution that is easy to sample from and $q(x)$ is difficult to sample from. By applying the MH Algorithm 10,000 times, and only sampling from $p(x)$, we can achieve the following result:</p> <p align="center"> <video controls="" width="560"> <source src="/assets/videos/metropolis-hastings-animation.mp4" type="video/mp4" /> </video> </p> <p>Effectively, samples from $p(x)$ and conclude that they are accepted as approximations of $q(x)$.</p> <h2 id="mathematical-background"> <a href="#mathematical-background" class="anchor-heading" aria-labelledby="mathematical-background"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Mathematical Background </h2> <p>Before formally defining the MH algorithm, I’d like to recall or introduce a few concepts that the reader should be familiar with to understand the proof of the MH algorithm.</p> <h3 id="monte-carlo"> <a href="#monte-carlo" class="anchor-heading" aria-labelledby="monte-carlo"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Monte Carlo </h3> <p>Monte Carlo methods are a class of computational techniques that use random sampling to approximate mathematical quantities, such as integrals. These methods are useful when analytical solutions are difficult or impossible to obtain.</p> <p>Given a probability density function $f(x)$ and independently and identically distributed (i.i.d) samples $X_1, X_2, …, X_n$ from $f(x)$, we can approximate the integral of a continuous function $a(x)$ with respect to $f(x)$:</p> \[I = \int a(x)f(x)\text{d}x\] <p>With the following estimate:</p> \[\hat{I} = \frac{1}{n} \sum_{i=1}^{n} a(x_i)\] <p>Where $\hat{I}$ is an unbiased estimator of $I$. By the law of large numbers $\hat{I} \rightarrow I$ and $\text{E}[\hat{I}] = I$</p> <p><strong>Proof:</strong></p> <p>It should be noted that by the Law of the Unconscious Statistician:</p> \[I = \int a(x)f(x)dx = \text{E}[a(x)]\] <p>By properties of expected value we apply the following:</p> \[\text{E}[\hat{I}] = \text{E}[\frac{1}{n} \sum_{i=1}^n a(x_i)] = \frac{1}{n} \sum_{i=1}^n \text{E}[a(x_i)]\] <p>Since $X_1, X_2, …, X_n$ are i.i.d</p> \[\text{E}[\hat{I}] = \frac{1}{n} (nI) = I = \text{E}[a(x)]\] <p><strong>QED</strong></p> <p>We can also show that as the number of samples increase, our estimate approaches the true value. \begin{proof}</p> \[\text{Var}[\hat{I}] = \text{Var}[\frac{1}{n} \sum_{i=1}^na(x_i)] = \frac{1}{n^2}\sum_i \text{Var}[a(x_i)]\] <p>Since our samples are i.i.d:</p> \[=\frac{1}{n^2}(n\text{Var}[a(x_i)]) = \frac{1}{n}\text{Var}[a(x_i)]\] <p>Now applying the limit:</p> \[\lim_{n \rightarrow \infty} \frac{1}{n}\text{Var}[a(x_i)] = 0\] <p><strong>QED</strong></p> <h2 id="markov-chain"> <a href="#markov-chain" class="anchor-heading" aria-labelledby="markov-chain"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Markov Chain </h2> <p>A Markov chain is a stochastic process where the future state depends only on the current state and not on the sequence of events that preceded it. A key concept in Markov chains is the <strong>stationary distribution</strong>, which is a probability distribution that remains unchanged as the chain evolves over time.</p> <p>The MC Stationary property requires the following:</p> <ol> <li>A p.d.f: $f(x)$</li> <li>An i.i.d sample $X_1, X_2, …, X_n$ drawn from $\Pr(\cdot \vert x_{i-1})$, the transition kernel from the previous state, $x_{i-1}$.</li> <li>The stationary distribution must satisfy the following process:</li> </ol> \[f(y) = \int \Pr(y \vert x) f(x) \text{dx}\] <p>If all three conditions are met, then the following is true:</p> <p>If the initial state $X_1$,​ is drawn from the stationary distribution $f(x)$, and each subsequent state $X_{i+1}$ is drawn from $\Pr⁡(⋅\vert x_i)$, then all states $X_i$ will also follow the stationary distribution $f(x)$.</p> <h2 id="markov-chain-monte-carlo"> <a href="#markov-chain-monte-carlo" class="anchor-heading" aria-labelledby="markov-chain-monte-carlo"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Markov Chain Monte Carlo </h2> <p>Markov Chain Monte Carlo (MCMC) methods combine Monte Carlo sampling with Markov chains to generate samples from a target probability distribution $f(x)$.</p> <p>Given a sequence of samples $X_1,X_2,…,X_n$ generated by an ergodic Markov chain with stationary distribution $f(x)$, we can approximate expectations of a function $a(x)$ as follows:</p> \[\mathbb{E}_f[a(x)] \approx \frac{1}{n} \sum_{i=1}^n a(x_i)\] <p>This approximation holds under the condition that the Markov chain will eventually produce samples that are distributed according to $f(x)$, even if the samples are not independent.</p> <p>By the definition above and the description above, the MH algorithm is an MCMC method.</p> <h2 id="metropolis-hastings-algorithm"> <a href="#metropolis-hastings-algorithm" class="anchor-heading" aria-labelledby="metropolis-hastings-algorithm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Metropolis-Hastings Algorithm </h2> <ol> <li>Pick $p(x)$ and $q(x)$ where $p(x)$ is a distribution that can be easily sampled from.</li> <li>Set an initial value of $x$ (could be anything, keep it reasonable)</li> <li>Take $x'$ from $p(x)$</li> <li>Evaluate $q(x')$</li> <li>Compute $\alpha(x', x) = \min(1, \frac{p(x')q(x|x')}{p(x)q(x'|x)})$</li> <li>Draw a random sample $u$ from $\text{Uniform}[0,1]$</li> <li> Accept or reject: If $u &lt; \alpha(x',x)$: $x_2 = x'$ ,else: $x_2 = x_1$ </li> </ol> <h2 id="proof-of-metropolis-hastings-algorithm"> <a href="#proof-of-metropolis-hastings-algorithm" class="anchor-heading" aria-labelledby="proof-of-metropolis-hastings-algorithm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Proof of Metropolis Hastings Algorithm </h2> <p>From the algorithm we know applying the MH Algorithm does the following:</p> <p align="center"> <img src="/assets/metropolis-movement.png" alt="MH Movement" width="400" /> </p> <p>The algorithm implies that for any $X_i$ and $X_{i+1}$ for $1 \leq i \leq n$ there must be a density $\Pr(X_{i+1} \vert X_i)$ that facilitates the movement of random variables.</p> <p>We also wish to prove $q(X_{i+1}) = \int \Pr(X_{i+1} \vert X_{i}) q(X_{i}) dX_{i}$ which is the stationary condition for our target distribution. However, we first need to find $\Pr(x_{n+1} \vert x_n)$ is before we can show the stationary condition holds.</p> <h3 id="finding-prx_n1-vert-x_n"> <a href="#finding-prx_n1-vert-x_n" class="anchor-heading" aria-labelledby="finding-prx_n1-vert-x_n"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Finding $\Pr(X_{n+1} \vert X_n)$: </h3> <p>$X_{n+1}$ can take on two values. Either $X_{n+1}$ takes on the values of $x’$ with some probability $w(x)$ or the sample is rejected in which case $X_{n+1} = x_n$. We can represent this as:</p> \[X_{n+1} = \begin{cases} x' \text{ determined by w(x)} \\ x_n \text{ 1- w(x)} \end{cases}\] <p>Since we are looking to determine $\Pr(X_{n+1} \vert X_n)$, using the Law of Total Probability, we get:</p> \[\Pr(X_{n+1} | X_n) = \text{Pr}(X_{n+1} = x' | X_{n} = x_{n} ) + (1- \text{Pr}(X_{n+1} = x' | X_{n} = x_{n} )) \cdot 1(X_{n+1} = X_n)\] <p>Where $1(X_{n+1} = X_n)$ is an indicator variable that tells us our sample was rejected. Let’s take $\Pr(X_{n+1} = x’ \vert X_{n} = x_n)$, the probability we move from one state to the next apart and quantify the result.</p> \[\Pr(X_{n+1} = x' | X_{n} = x_n) = \Pr(X' | X_n = x_n) \cdot \Pr(U \leq \alpha(x', x_n) | X' = x', X_n =x_n) =q(x' | x_n) \alpha(x', x_n)\] <p>Since our outcomes are binary, $X_{n+1}$ either takes on $x’$ or $X_n$, we can directly construct our density function:</p> \[\text{Pr}(X_{n+1} | X_{n}) = w(x)f(x_{n+1} | x_n) + (1-w(x))1(X_{n+1} = X_n)\] <p>where $w(x)$ is:</p> \[w(x_n) = \int q(x_{n+1} | x_n) \alpha(x_{n+1}, x_n)dx_{n+1}\] <p>and $f(x_{n+1} \vert x_n)$ is:</p> \[f(x_{n+1} | x_n) = \frac{q(x_{n+1} | x_n) \alpha(x_{n+1}, x_n)}{\int q(x_{n+1}|x_n)\alpha(x_{n+1} , x_n) dx_{n+1} }\] <p>We’ll now show the transition distribution satisfies the stationary distribution. We’d like to show the density kernel we just constructed converges to the target distribution:</p> \[q(X_{n+1}) = \int \Pr(X_{n+1} | X_n)q(X_n)dX_{_n}\] <p>Here we assume our density is ergodic where detailed balance is a sufficient condition to show there exists a stationary distribution. Detailed Balance tells us the rate of transition given we start at $x’$ and move to $x_n$ must be equal to the rate of transition given we start at $x_n$ and move to $x’$. For the Metropolis Hastings, this looks like:</p> \[q(x')\Pr(X_{n+1} = x'| X_n = x_n) = q(x_n)\Pr(X_{n+1} = x_n | X_{n} = x')\] <p>Claim: The Metropolis Hastings algorithm satisfies detailed balance. From the previous section we know the probability of transition is:</p> \[\Pr(X_{n+1} = x'| X_n = x_n) = p(x'|x_n)\alpha(x',x_n) =p(x'|x_n) \min \{ 1, \frac{q(x')p(x_n|x')}{q(x_n)p(x'|x_n)} \}\] <p>We can expand the RHS further:</p> \[=\min\{p(x'|x_n), \frac{q(x')p(x_n|x')}{q(x_n)}\}\] <p>If we multiply both sides by $q(x_n)$, we get:</p> \[\Pr(x'|x_n)q(x_n)= \min ( q(x_n)p(x'|x_n),q(x')p(x_n|x'))\] \[\Rightarrow \Pr(x'|x_n)q(x_n) = \Pr(x_n|x')q(x')\] <p>The Metropolis Hastings Algorithm satisfies detailed balance.</p> <p>Finally, we can integrate both sides w.r.t $x_n$ and find:</p> \[q(x') = \int \Pr(x' |x_n)q(x_n)dx_n\] \[\Rightarrow q(x_{n+1}) = \int \Pr(x_{n+1} |x_n)q(x_n)dx_n\] </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
