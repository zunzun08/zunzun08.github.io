<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/zunzun08.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/zunzun08.github.io/" rel="alternate" type="text/html" /><updated>2025-05-26T14:56:41-05:00</updated><id>http://localhost:4000/zunzun08.github.io/feed.xml</id><title type="html">zunzun08.github.io</title><entry><title type="html">Rejection Sampling</title><link href="http://localhost:4000/zunzun08.github.io/2025/05/19/rejection-sampling.html" rel="alternate" type="text/html" title="Rejection Sampling" /><published>2025-05-19T00:00:00-05:00</published><updated>2025-05-19T00:00:00-05:00</updated><id>http://localhost:4000/zunzun08.github.io/2025/05/19/rejection-sampling</id><content type="html" xml:base="http://localhost:4000/zunzun08.github.io/2025/05/19/rejection-sampling.html"><![CDATA[<!--
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['\\(','\\)'], ['$', '$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
-->
<h2 id="overview">Overview</h2>
<p>Rejection Sampling is an algorithm that provides us with an intuitive way to sample random variables from a <strong>target distribution</strong>, $q(x)$, that cannot be sampled from directly but whose function form is known. We do so by using a <strong>proposal distribution</strong>, $p(x)$, that can be sampled from directly. The rejection sampling algorithm requires samples drawn from $p(x)$ to be evaluated on a probability the sample can be assumed to have been drawn from $q(x)$. If the sample is not assumed to be drawn from $q(x)$, the sample is rejected and the process is repeated. The underlying acceptance of the sample drawn from $p(x)$ relies on the overlap between $p(x)$ and $q(x)$. If $p(x)$ and $q(x)$ are similar distributions then we say the two distributions have high overlap and samples drawn from either distribution could be said to have been drawn from the other. The opposite is true for distributions with low overlap. Rejection sampling provides us with an introductory understanding of overlap as we define the likelihood samples can replace each other with the constant $M$:</p>

\[M = \sup_{x \in \Omega} \frac{q(x)}{p(x)} &lt; \infty\]

<p>We note that $M \geq 1$:
<br />
<strong>Proof:</strong>
<br />
Suppose $ M &lt; 1 $.</p>

<p>Since $M$ is the supremum of $\frac{q(x)}{p(x)}$, then for all $x$:
\(M \geq \frac{q(x)}{p(x)} \implies M p(x) \geq q(x).\)</p>

<p>Integrating both sides:
\(M \int p(x) \, dx \geq \int q(x) \, dx.\)
Since $ p(x) $ and $ q(x) $ are p.d.f.s, $ \int p(x) \, dx = \int q(x) \, dx = 1 $. Thus:</p>

\[M \geq 1,\]

<p>which contradicts the assumption $ M &lt; 1 $.</p>

<p>Since $M \geq 1$, we know the following must be true for $q(x)$:
<br /></p>
<ol>
  <li>$q(x) \geq p(x) \space  \forall x \in \Omega$
<br /></li>
  <li>$p(x)$ and $q(x)$ share the same $\Omega$</li>
</ol>

<p>In rejection sampling we define overlap with $M$ since $M$ serves as a measure of similarity between $p(x)$ and $q(x)$. If $M$ is very close to 1, $p(x)$ and $q(x)$ are quite similar. if M is very close to 0, $p(x)$ and $q(x)$ are not similar. Note, a downside to rejection sampling is that $M$ must exist which heavily constrains the number of density functions we can apply rejection sampling sampling to. However, we know based on what was said above, the following figure shows us the relationship between $p(x)$ and $q(x)$.</p>

<p>Once we’ve determined $M$, we can generate samples $X_1,X_2,…,X_n$ from $p(x)$ and apply the rejection sampling algorithm. We will show the accepted samples will then follow the target distribution $q(x)$.</p>

<p>We can condense the above into the followin algorithm:</p>

<p>1.Draw a sample $X \sim p(x)$</p>

<p>2.Draw a sample $u \sim U[0,1]$</p>

<p>3.If
$u &lt; \frac{q(x)}{M*p(x)}$ then $X \sim q$,  else: Discard $X$ and $u$</p>

<p>4.Repeat</p>

<p>Here is an example of the algorithm implemented in Python:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
import scipy
def rejection_sampling(num_samples, M, prop_dist, target_dist):
    samples = []
    for _ in range(num_samples):
            x = prop_dist.rvs()  #Random variable from our proposed distribution
            u = scipy.stats.uniform.rvs()    #Random variable from our uniform distribution
                
            #Ratio is the realized random variable evaluated at both the proposed and the target distributions

            ratio = target_dist.pdf(x) / (prop_dist.pdf(x)*M)
            
            
            if u &lt;= (ratio):  # Accept-reject condition
                
                samples.append(x)
    return np.array(samples)
</code></pre></div></div>
<h3 id="example">Example</h3>
<p>Here we use $p \sim \text{Exp}(\lambda = \frac{1}{2})$ to target a standard exponential distribution:</p>

<p><img src="/assets/rejection_sampling_example.png" alt="RS Exp Example" /></p>

<h3 id="proof-of-rejection-sampling">Proof of Rejection Sampling</h3>
<p>Our goal is to show that if we follow the rejection sample algorithm, the random variable we drew $X$ from $p(x)$, can be said $X \sim q(x)$. Probability theory tells us that if  $X \sim q(x)$, the following are equivalent:</p>

\[\Pr(X \in A) = \int_A q(x)\text{dx} = Q(x)\]

<p>Where $A$ is sample space of $q(x)$. This will be the idea we wish to prove.</p>

<p>When we run the rejection sampling algorithm, there exists some probability we accept or reject the sample. This probability is random before the algorithm begins and based on the fact that we accept or reject, the outcome is binary. Whether or not the outcome occurs for every iteration of the algorithm is a random variable (r.v) that we call $Z$ where $Z$. We’ve defined the conditions when a sample $X$ is accepted rejected in the algorithm and it is based on the inequality:</p>

<p>$U &lt; \frac{q(x)}{M*p(x)}$</p>

<p>It follows that since M is the supremum of $\frac{q(x)}{p(x)}$, if M exists then $\frac{q(x)}{M*p(x)}$ is bounded on the interval of $[0,1]$ for all $x$ in the support of $p(x)$ and $q(x)$. We can now assign the probability the event occurs to our new random variable $Z$ as:
\(Z \sim  \text{Bernoulli}(\frac{q(x)}{Mp(x)})\)</p>

<p>Now that $Z$ is well defined, we turn our attention to the following:</p>

\[X \space | \space Z=1\]

<p>Essentially, what is the distribution of $X$ after we’ve accepted the sample? We want to show that given we’ve accepted our sample, we can say $X|Z=1 \sim q(x)$. To do so we’ll need the following:
\(\Pr(X \in A | Z=1) = Q(x)\)
where $A$ is the sample space of $q(x)$.</p>

<table>
  <tbody>
    <tr>
      <td>Using Bayes’ Theorem we can rewrite $\Pr(X \in A</td>
      <td>Z=1)$ as:</td>
    </tr>
  </tbody>
</table>

\[\Pr(X \in A | Z=1) = \frac{\Pr(Z = 1 | X \in A) \cdot \Pr(X \in A)}{\Pr(Z=1)}\]

<p>Lets turn our attention to the numerator. Since we have a conditional probability, we can make the following observation:</p>

\[\Pr(Z = 1 | X \in A) = \frac{\Pr(Z=1 \cap X \in A)}{\Pr(X \in A)}\]

<p>By multiplying both sides by $\Pr(X \in A)$ and plugging the right hand side into the numerator we arrive at:</p>

\[\Pr(X \in A | Z=1) = \frac{\Pr(Z=1 \cap X \in A)}{\Pr(Z=1)}\]

<p>Looking deeper into $\Pr(Z=1 \cap K \in A)$, this is something we can evaluate:
By the law of Total Probability:</p>

\[\Pr(Z=1 \cap K \in A) = \int_{A} (U &lt; \frac{q(x)}{Mp(x)}) \cdot p(x)\text{dx} = \iint_{0}^{1} (U &lt; \frac{q(x)}{Mp(x)}) \cdot p(x) \text{dx} = \int_{-\infty}^{\infty} \frac{q(x)}{Mp(x)}\cdot p(x) \text{dx}\]

\[\Rightarrow \int_{-\infty}^{\infty} \frac{q(x)}{M}\text{dx} = \frac{1}{M} \int_{-\infty}^{\infty}q(x)\text{dx} = \frac{1}{M} \Pr(X \in A) = \frac{1}{M} \cdot Q(x)\]

<p>Now we turn our focus to the denominator $\Pr(Z=1)$.</p>

\[\Pr(Z=1) = \int_{-\infty}^{\infty} 1(U &lt; \frac{q(x)}{Mp(x)}) p(x) \text{dx} = \int_{-\infty}^{\infty} \frac{q(x)}{Mp(x)}p(x) \text{dx} = \frac{1}{M} \int_{-\infty}^{\infty} q(x)\text{dx} = \frac{1}{M} (1) = \frac{1}{M}\]

<p>Finishing up we arrive at out final result:</p>

\[\Pr(X \in A | Z=1) = \frac{\frac{1}{M} \Pr(X \in A)}{\frac{1}{M}} = \Pr(X \in A) = Q(x)\]

<p>Therefore, given we accept the random variable we sampled, we’ve shown $X \sim q(x)$.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Overview Rejection Sampling is an algorithm that provides us with an intuitive way to sample random variables from a target distribution, $q(x)$, that cannot be sampled from directly but whose function form is known. We do so by using a proposal distribution, $p(x)$, that can be sampled from directly. The rejection sampling algorithm requires samples drawn from $p(x)$ to be evaluated on a probability the sample can be assumed to have been drawn from $q(x)$. If the sample is not assumed to be drawn from $q(x)$, the sample is rejected and the process is repeated. The underlying acceptance of the sample drawn from $p(x)$ relies on the overlap between $p(x)$ and $q(x)$. If $p(x)$ and $q(x)$ are similar distributions then we say the two distributions have high overlap and samples drawn from either distribution could be said to have been drawn from the other. The opposite is true for distributions with low overlap. Rejection sampling provides us with an introductory understanding of overlap as we define the likelihood samples can replace each other with the constant $M$:]]></summary></entry><entry><title type="html">Metropolis Hastings</title><link href="http://localhost:4000/zunzun08.github.io/2025/05/18/metropolis-hastings.html" rel="alternate" type="text/html" title="Metropolis Hastings" /><published>2025-05-18T00:00:00-05:00</published><updated>2025-05-18T00:00:00-05:00</updated><id>http://localhost:4000/zunzun08.github.io/2025/05/18/metropolis-hastings</id><content type="html" xml:base="http://localhost:4000/zunzun08.github.io/2025/05/18/metropolis-hastings.html"><![CDATA[<!--
---
title: "Metropolis-Hastings Algorithm"
layout: post
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['\\(','\\)'], ['$', '$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
-->

<h1 id="the-metropolis-hastings-algorithm">The Metropolis Hastings Algorithm</h1>

<h2 id="introduction">Introduction</h2>
<p>The Metropolis-Hastings (MH) algorithm is a Monte Carlo Markov Chain (MCMC) method that allows us to generate samples from a target distribution. A target distribution is a function whose form is known but whose constant of proportionality is unknown and/or cannot be calculated through traditional methods or numerical methods. Because the constant of proportionality cannot be calculated, traditional methods of sampling such as the inverse CDF method cannot be applied. We’ll assume the rejection sampling method does not work here for our proposal and target distributions. Instead, we turn to the Metropolis Hastings algorithm which works by also works by an iterative process. The Metropolis Hastings algorithm draws samples from a proposal probability density function and based on an overlap measure, the sample is accepted or rejected. After some number of iterations, the samples accepted by MH algorithm follow target distribution, without ever calculating the constant of integration.</p>

<h3 id="example">Example</h3>
<p>Suppose $q(x) \sim N(0,1)$ and $p(x) \sim N(1,2)$ where $p(x)$ is a distribution that is easy to sample from and $q(x)$ is difficult to sample from. By applying the MH Algorithm 10,000 times, and only sampling from $p(x)$, we can achieve the following result:</p>

<p><img src="/assets/metropolis-hastings-animation.mp4" alt="MH Example Video" /></p>

<p>Effectively, samples from $p(x)$ and concluding that they are accepted as approximations of $q(x)$.</p>

<h2 id="mathematical-background">Mathematical Background</h2>
<p>Before formally defining the Metropolis Hastings, I’d like to recall or introduce a few concepts the reader should be familiar with to understand the proof of the Metropolis-Hastings Algorithm.</p>

<h3 id="monte-carlo">Monte Carlo</h3>
<p>Monte Carlo methods are a class of computational techniques that use random sampling to approximate mathematical quantities, such as integrals. These methods are useful when analytical solutions are difficult or impossible to obtain.</p>

<p>Given a probability density function $f(x)$ and independently and identically distributed (i.i.d) samples $x_1, x_2, …, x_n$ from $f(x)$, we can approximate the integral of a continuous function $a(x)$ with respect to $f(x)$:</p>

\[I = \int a(x)f(x)\text{d}x\]

<p>With the following estimation:</p>

\[\hat{I} = \frac{1}{n} \sum_{i=1}^{n} a(x_i)\]

<p>Where $\hat{I}$ is an unbiased estimator of $I$. By the law of large numbers $\hat{I} \rightarrow I$ and $\text{E}[\hat{I}] = I$</p>

<p><strong>Proof:</strong></p>

<p>It should be noted that by the Law of the Unconscious Statistician:</p>

\[I = \int a(x)f(x)dx = \text{E}[a(x)]\]

<p>By properties of expected value we apply the following:</p>

\[\text{E}[\hat{I}] = \text{E}[\frac{1}{n} \sum_{i=1}^n a(x_i)] = \frac{1}{n} \sum_{i=1}^n \text{E}[a(x_i)]\]

<p>Since $x_1, x_2, …, x_n$ are i.i.d</p>

\[\text{E}[\hat{I}] = \frac{1}{n} (nI) = I = \text{E}[a(x)]\]

<p><strong>QED</strong></p>

<p>We can also show that as the number of samples increase, our estimate approaches the true value.
\begin{proof}</p>

\[\text{Var}[\hat{I}] = \text{Var}[\frac{1}{n} \sum_{i=1}^na(x_i)] = \frac{1}{n^2}\sum_i \text{Var}[a(x_i)]\]

<p>Since our samples are i.i.d:</p>

\[=\frac{1}{n^2}(n\text{Var}[a(x_i)]) = \frac{1}{n}\text{Var}[a(x_i)]\]

<p>Now applying the limit:</p>

<p>\(\lim_{n \rightarrow \infty} \frac{1}{n}\text{Var}[a(x_i)] = 0\)
\end{proof}
\subsubsection{Markov Chain}</p>

<p>A Markov chain is a stochastic process where the future state depends only on the current state and not on the sequence of events that preceded it. A key concept in Markov chains is the <strong>stationary distribution</strong>, which is a probability distribution that remains unchanged as the chain evolves over time.</p>

<p>The MC Stationary property requires the following:</p>
<ol>
  <li>A p.d.f: $f(x)$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>An i.i.d sample $x_1, x_2, …, x_n$ drawn from $\Pr(\cdot</td>
          <td>x_{i-1})$, the transition kernel from the previous state, $x_{i-1}$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>The stationary distribution must satisfy the following process:</li>
</ol>

\[f(y) = \int \Pr(y | x) f(x) \text{dx}\]

<p>If all three conditions are met then the following is true:
If the initial state $<code class="language-plaintext highlighter-rouge">x_1</code>$,​ is drawn from the stationary distribution $<code class="language-plaintext highlighter-rouge">f(x)</code>$, and each subsequent state $<code class="language-plaintext highlighter-rouge">x_{i+1}</code>$ is drawn from $<code class="language-plaintext highlighter-rouge">\Pr⁡(⋅∣x_i)</code>$, then all states $<code class="language-plaintext highlighter-rouge">x_i</code>$ will also follow the stationary distribution $<code class="language-plaintext highlighter-rouge">f(x)</code>$.</p>

<p>\subsubsection{Markov Chain Monte Carlo}</p>

<p>Markov Chain Monte Carlo (MCMC) methods combine Monte Carlo sampling with Markov chains to generate samples from a target probability distribution $<code class="language-plaintext highlighter-rouge">f(x)</code>$.</p>

<p>Given a sequence of samples $x_1,x_2,…,x_n$ generated by an ergodic Markov chain with stationary distribution $<code class="language-plaintext highlighter-rouge">f(x)</code>$, we can approximate expectations of a function $<code class="language-plaintext highlighter-rouge">a(x)</code>$ as follows:</p>

\[\mathbb{E}_f[a(x)] \approx \frac{1}{n} \sum_{i=1}^n a(x_i)\]

<p>This approximation holds under the condition that the Markov chain will eventually produce samples that are distributed according to $<code class="language-plaintext highlighter-rouge">f(x)</code>$, even if the samples are not independent.</p>

<p>By the definition above and the description above, the MH algorithm is an MCMC method.</p>

<p>\subsection{Metropolis-Hastings Algorithm}
\begin{enumerate}
\item Pick $p(x)$ and $q(x)$ where $p(x)$ is a distribution that can be easily sampled from.
\item Set an initial value of $x$ (could be anything, keep it reasonable)
\item Take $x’$ from $p(x)$
\item Evaluate $q(x’)$
\item Compute 
$\alpha(x’, x) = \min(1, \frac{p(x’)q(x|x’)}{p(x)q(x’|x)})$</p>

<p>\item Draw a random sample $u$ from $\text{Uniform}[0,1]$
\item Accept or reject: 
   If $u &lt; \alpha(x’,x)$:
	$x_2 = x’$
	,else:
		$x_2 = x_1$
\end{enumerate}</p>

<p>\newpage
\subsection{Proof of Metropolis Hastings Algorithm}
From the algorithm we know applying the MH Algorithm does the following:</p>

<p>\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{metropolis Movement.png}
    \caption{Metropolis Hastings Steps}
    \label{fig:MH movement}
\end{figure}</p>

<table>
  <tbody>
    <tr>
      <td>This implies that for any $X_i$ and $X_{i+1}$ for $1 \leq i \leq n$ there must be a density $\text{Pr}(X_{i+1}</td>
      <td>X_i)$ that facilitates the movement of random variables.</td>
    </tr>
  </tbody>
</table>

<p>We also wish to prove $q(X_{i+1}) = \int \Pr(X_{i+1} | X_{i}) q(X_{i}) dX_{i}$ which is the stationary condition for our target distribution.
However, we first need to find $\Pr(x_{n+1} | x_n)$ is before we can show the stationary condition holds.
<br />
Finding $\Pr(X_{n+1} | X_n)$:
$X_{n+1}$ can take on two values. Either $X_{n+1}$ takes on the values of $x’$ with some probability w(x) or the sample is rejected in which case $X_{n+1} = x_n$. 
We can represent this as:</p>

\[X_{n+1} = \begin{cases}
x' \text{ determined by w(x)} \\
x_n \text{ 1- w(x)}
\end{cases}\]

<table>
  <tbody>
    <tr>
      <td>Since we are looking to determine $\Pr(X_{n+1}</td>
      <td>X_n)$, using the Law of Total Probability, we get:</td>
    </tr>
  </tbody>
</table>

\[\Pr(X_{n+1} | X_n) = \text{Pr}(X_{n+1} = x' | X_{n} = x_{n} ) + (1- \text{Pr}(X_{n+1} = x' | X_{n} = x_{n} )) \cdot 1(X_{n+1} = X_n)\]

<table>
  <tbody>
    <tr>
      <td>Where $1(X_{n+1} = X_n)$ is an indicator variable that tells us our sample was rejected. Let’s take  $\Pr(X_{n+1} = x’</td>
      <td>X_{n} = x_n)$, the probability we move from one state to the next apart and quantify the result.</td>
    </tr>
  </tbody>
</table>

\[\Pr(X_{n+1} = x' | X_{n} = x_n) = \Pr(X' | X_n = x_n) \cdot \Pr(U \leq \alpha(x', x_n) | X' = x', X_n =x_n) =q(x' | x_n) \alpha(x', x_n)\]

<p>Since our outcomes are binary, $X_{n+1}$ either takes on $x’$ or $X_n$, we can directly construct our density function:</p>

\[\text{Pr}(X_{n+1} | X_{n}) = w(x)f(x_{n+1} | x_n) + (1-w(x))1(X_{n+1} = X_n)\]

<p>where $w(x)$ is:</p>

\[w(x_n) = \int q(x_{n+1} | x_n) \alpha(x_{n+1}, x_n)dx_{n+1}\]

<table>
  <tbody>
    <tr>
      <td>and $f(x_{n+1}</td>
      <td>x_n)$ is:</td>
    </tr>
  </tbody>
</table>

\[f(x_{n+1} | x_n) = \frac{q(x_{n+1} | x_n) \alpha(x_{n+1}, x_n)}{\int q(x_{n+1}|x_n)\alpha(x_{n+1} , x_n) dx_{n+1}
}\]

<p>We’ll now show the transition distribution satisfies the stationary distribution. 
We’d like to show the density kernel we just constructed converges to the target distribution:</p>

\[q(X_{n+1}) = \int \Pr(X_{n+1} | X_n)q(X_n)dX_{_n}\]

<p>Here we assume our density is ergodic where detailed balance is a sufficient condition to show there exists a stationary distribution.
Detailed Balance tells us the rate of transition given we start at $x’$ and move to $x_n$ must be equal to the rate of transition given we start at $x_n$ and move to $x’$. For the Metropolis Hastings, this looks like:</p>

\[q(x')\Pr(X_{n+1} = x'| X_n = x_n) = q(x_n)\Pr(X_{n+1} = x_n | X_{n} = x')\]

<p>Claim: The Metropolis Hastings algorithm satisfies detailed balance.
From the previous section we know the probability of transition is:</p>

\[\Pr(X_{n+1} = x'| X_n = x_n) = p(x'|x_n)\alpha(x',x_n) =p(x'|x_n) \min \{ 1, \frac{q(x')p(x_n|x')}{q(x_n)p(x'|x_n)} \}\]

<p>We can expand the RHS further:</p>

\[=\min\{p(x'|x_n), \frac{q(x')p(x_n|x')}{q(x_n)}\}\]

<p>If we multiply both sides by $q(x_n)$, we get:</p>

\[\Pr(x'|x_n)q(x_n)= \min ( q(x_n)p(x'|x_n),q(x')p(x_n|x'))\]

\[\Rightarrow \Pr(x'|x_n)q(x_n) = \Pr(x_n|x')q(x')\]

<p>Thus, the Metropolis Hastings Algorithm satisfies detailed balance.
We can integrate both sides w.r.t $x_n$ and find:
\(q(x') = \int \Pr(x' |x_n)q(x_n)dx_n\)
\(\Rightarrow
q(x_{n+1}) = \int \Pr(x_{n+1} |x_n)q(x_n)dx_n\)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– title: “Metropolis-Hastings Algorithm” layout: post —]]></summary></entry></feed>